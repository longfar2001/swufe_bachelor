\NeedsTeXFormat{LaTeX2e}% LaTeX 2.09 can't be used (nor non-LaTeX)
[1994/12/01]% LaTeX date must December 1994 or later
\documentclass[6pt]{article}
\pagestyle{headings}
\setlength{\textwidth}{18cm}
\setlength{\topmargin}{0in}
\setlength{\headsep}{0in}

\title{Introduction to PDEs, Fall 2022}
\author{\textbf{Homework 5} Due Nov 3}
\date{}

\voffset -1.25cm \hoffset -3.5cm \textwidth 18cm \textheight 25cm
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{gensymb}

  \usepackage{paralist}
  \usepackage{graphics} %% add this and next lines if pictures should be in esp format
  \usepackage{epsfig} %For pictures: screened artwork should be set up with an 85 or 100 line screen
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf}%This is to transfer .eps figure to .pdf figure; please compile your paper using PDFLeTex or PDFTeXify.
 \usepackage[colorlinks=true]{hyperref}
 \usepackage{multirow}
\input{amssym.tex}
\def\N{{\Bbb N}}
\def\Z{{\Bbb Z}}
\def\Q{{\Bbb Q}}
\def\R{{\Bbb R}}
\def\C{{\Bbb C}}
\def\SS{{\Bbb S}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}
%\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{solution}{Solution}
%\newtheorem{proof}{Proof}
 \numberwithin{equation}{section}
%\newtheorem*{problem}{Problem}
%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}
%\newtheorem*{notation}{Notation}
\newcommand{\ep}{\varepsilon}
\newcommand{\eps}[1]{{#1}_{\varepsilon}}
\newcommand{\keywords}

\def\bb{\begin}
\def\bc{\begin{center}}       \def\ec{\end{center}}
\def\ba{\begin{array}}        \def\ea{\end{array}}
\def\be{\begin{equation}}     \def\ee{\end{equation}}
\def\bea{\begin{eqnarray}}    \def\eea{\end{eqnarray}}
\def\beaa{\begin{eqnarray*}}  \def\eeaa{\end{eqnarray*}}
\def\hh{\!\!\!\!}             \def\EM{\hh &   &\hh}
\def\EQ{\hh & = & \hh}        \def\EE{\hh & \equiv & \hh}
\def\LE{\hh & \le & \hh}      \def\GE{\hh & \ge & \hh}
\def\LT{\hh & < & \hh}        \def\GT{\hh & > & \hh}
\def\NE{\hh & \ne & \hh}      \def\AND#1{\hh & #1 & \hh}

\def\r{\right}
\def\lf{\left}
\def\hs{\hspace{0.5cm}}
\def\dint{\displaystyle\int}
\def\dlim{\displaystyle\lim}
\def\dsup{\displaystyle\sup}
\def\dmin{\displaystyle\min}
\def\dmax{\displaystyle\max}
\def\dinf{\displaystyle\inf}

\def\al{\alpha}               \def\bt{\beta}
\def\ep{\varepsilon}
\def\la{\lambda}              \def\vp{\varphi}
\def\da{\delta}               \def\th{\theta}
\def\vth{\vartheta}           \def\nn{\nonumber}
\def\oo{\infty}
\def\dd{\cdots}               \def\pa{\partial}
\def\q{\quad}                 \def\qq{\qquad}
\def\dx{{\dot x}}             \def\ddx{{\ddot x}}
\def\f{\frac}                 \def\fa{\forall\,}
\def\z{\left}                 \def\y{\right}
\def\w{\omega}                \def\bs{\backslash}
\def\ga{\gamma}               \def\si{\sigma}
\def\iint{\int\!\!\!\!\int}
\def\dfrac#1#2{\frac{\displaystyle {#1}}{\displaystyle {#2}}}
\def\mathbb{\Bbb}
\def\bl{\Bigl}
\def\br{\Bigr}
\def\Real{\R}
\def\Proof{\noindent{\bf Proof}\quad}
\def\qed{\hfill$\square$\smallskip}

\begin{document}
\maketitle

\textbf{Name}:\rule{1 in}{0.001 in} \\
\begin{enumerate}

\item The Sturm--Liouville theory applies to the linear ordinary differential equations of the general form
\[\frac{d}{d x}\left[p(x) \frac{d y}{d x}\right]+q(x) y=-\lambda w(x) y,\]
where $p,w$ are assumed to be of the same sign, $p,p',q$ and $w$ are continuous functions of $x$.  If this equation is endowed with the following boundary conditions
\[
\begin{array}{ll}
\alpha_1 y(a)+\alpha_2 y^{\prime}(a)=0 & \alpha_1^2+\alpha_2^2>0, \\
\beta_1 y(b)+\beta_2 y^{\prime}(b)=0 & \beta_1^2+\beta_2^2>0,
\end{array}
\]
then the statements in our class hold.

Let $(y_i,\lambda_i)$, $i=1,2$, be two eigen-pairs of the problem with $\lambda_1\neq\lambda_2$.  Prove that
\[\left\langle y_1, y_2\right\rangle:=\int_a^b y_1(x) y_2(x) w(x) d x=0.\]

Remark:  Indeed, one can show that
\[\left\langle y_n, y_m\right\rangle=\int_a^b y_n(x) y_m(x) w(x) d x=\delta_{m n},\]
where $\delta_{m n}$ is the Kronecker delta function.

\item Let us recall the following approach in class to tackle a problem with inhomogeneous boundary conditions: to apply the Sturm--Liouville Theorem, one first needs to take care of the corresponding eigenvalue problem and find the corresponding eigenfunctions.  In almost all the applications we can see in a bounded domain, homogeneous DBC, NBC, and RBC are the three main types, the EPs of which are already studied by you in a previous HW problem.  This is enough for this course, and it helps a lot if you know these eigenfunctions by heart (well, it takes practice anyhow), e.g., $\sin\frac{k\pi x}{L}$ for DBC, $\cos\frac{k\pi x}{L}$ for NBC, etc.

However, when the boundary conditions are not homogeneous, we need to convert the problem into one with homogeneous BC as mentioned in class.  Consider
\begin{equation}\label{mu12}
\left\{
\begin{array}{ll}
u_t=Du_{xx},& x\in(0,L),t\in\mathbb R^+,\\
u(x,0)=\phi(x),&x\in(0,L),\\
u(0,t)=\mu_1(t), u(L,t)=\mu_2(t), &t\in\mathbb R^+.
\end{array}
\right.
\end{equation}
Note that the BC is DBC, but we can not simply write $u=\sum C_k(t)\sin\frac{k\pi x}{L}$ since the BC is not homogeneous (it is easy to check that this series can not satisfy the BC for general functions $\mu_i(t)$).  Therefore, we introduce $\tilde u(x,t)=u(x,t)+w(x,t)$ for some specific $w$ to be chosen such that $\tilde u$ satisfies the homogeneous DBC, i.e., $\tilde u(0,t)=\tilde u(L,t)=0$ for any $t$.  It is easy to see that we must restrict $w$ such that $w(0,t)=-\mu_1(t)$ and $w(L,t)=-\mu_2(t)$.  A natural choice of such $w$ is $w(x,t)=\frac{x-L}{L}\mu_1(t)-\frac{x}{L}\mu_2(t)$.

i) work the problem for $\tilde u(x,t)$ as in class and then finish solving for $u(x,t)$ in terms of infinite series;

ii) again, the choice of such $w$ is not unique; can you give an explicit form of another such $w(x,t)$?  The motivation behind this is that there are a lot of such choices, but it seems the one that I proposed is the simplest (I would be happy to see that I am wrong).

\item  Let us revisit the following IBVP
\begin{equation}
\left\{
\begin{array}{ll}
u_t=u_{xx},& x\in(0,\pi),t\in\mathbb R^+,\\
u(x,0)=x,&x\in(0,\pi),\\
u(0,t)=\sin t, u(L,t)=\cos t, &t\in\mathbb R^+.
\end{array}
\right.
\end{equation}
This problem is to numerically test that the solution is independent of the choice of $w(x,t)$.

1) choose the first transformation function as above $w^{(1)}(x,t)=\frac{\pi-x}{\pi}\sin t+\frac{x}{\pi}\cos t$, and then find the solution, denoted by $u^{(1)}(x,t)$, in terms of infinite series;

2) pick an alternative $w^{(2)}(x,t)$ of your own choice and then find the corresponding $u^{(2)}(x,t)$;

3) plot $u^{(1)}(x,t)$ and $u^{(2)}(x,t)$ with truncated $N$ for several $t$, say $N=10$, probably you want to test first that $N=10$ is large enough as previous HWs.  Then show that $u^{(1)}(x,t)$ and $u^{(2)}(x,t)$ are the same for all time;

4) try to prove that $u^{(1)}(x,t)$ and $u^{(2)}(x,t)$ equal analytically.

\item Let us work on the following cousin problem of (\ref{mu12})
\begin{equation}
\left\{
\begin{array}{ll}
u_t=Du_{xx},& x\in(0,L),t\in\mathbb R^+,\\
u(x,0)=\phi(x),&x\in(0,L),\\
u_x(0,t)=\mu_1(t), u_x(L,t)=\mu_2(t), &t\in\mathbb R^+.
\end{array}
\right.
\end{equation}
First convert this problem into one with homogeneous NBC.  Then solve the resulting problem and then write $u(x,t)$ in infinite series.

\item Solve the following IBVP
\begin{equation}\label{4}
\left\{
\begin{array}{ll}
u_t=Du_{xx}+x-\pi,& x\in(0,\pi),t\in\mathbb R^+,\\
u(x,0)=\pi-x,&x\in(0,\pi),\\
u(0,t)=\pi, u(\pi,t)=0, &t\in\mathbb R^+;
\end{array}
\right.
\end{equation}
Let $D=1$.  Then plot $u^{10}$ at $t=1,2,5,10$ etc to illustrate the large time behavior of the solution.

\item Consider the following IBVP
\begin{equation}\label{53}
\left\{
\begin{array}{ll}
u_t=u_{xx},& x\in(0,1),t>0,\\
u(x,0)=0, x\in(0,\frac{1}{2}); u(x,0)=1, x\in[\frac{1}{2},1],\\
u(0,t)=1, u(1,t)=2, &t>0.
\end{array}
\right.
\end{equation}

(i).  Without solving the problem, state what is the solution or shape of $u(x,t)$ as $t\rightarrow \infty$.  Hint: use your physical intuition.  It should be a function independent of time $t$ and we call it a \emph{steady state}.

(ii).  Solve (\ref{53}) in terms of infinite series.  Plot $u^{10}(x,t)$ for $t=0, t=0.01$, $t=0.05$ and $t=0.1$ over $(0,1)$ on the same graph.  You should observe that the initially discontinuity at $x=0$ is smeared out right away.  Then plot $u^{10}(x,t)$ for $t=10$ and the steady state in (i) on the same graph.

(iii).  Send $t$ to $\infty$ to rigorously confirm your observations in (ii).

\item Let us consider cooking a meatball in a boiling hot pot.  Assumptions we make are:

A1.) \emph{the meatball is perfectly round with a radius} $R$;

A2.) \emph{the meatball is solid and homogeneous};

A3.) \emph{the ball is well dipped into the water that is boiling at a constant temperature} (say 100$^{\circ}$;)

Suppose that the meatball is of uniform temperature initially, say 25$^{\circ}$.  We say that a meatball is cooked if the temperature of its center reaches a certain value, say 70 $^{\circ}$ (the value itself does not matter much for this problem).
Ask your parents or whoever that cooks the following questions (just use the gut feeling):

i) suppose that it takes 10 mins to cook a meatball of weight 50g.  How much time does it take to cook a meatball of weight 100g under the same condition? shorter than, is, or longer than 10 mins?

ii) suppose that it takes 10 mins to cook a meatball of radius 1cm.  How much time does it take to cook a meatball of radius 2cm under the same condition?  shorter than, is, or longer than 10 mins?

\item Now we PDE the hotpot.  Let $u(x,t)$ be the temperature of a meatball at location $x=(x_1,x_2,x_3)$ and time $t$, then its cooking follows
\begin{equation}\label{hotpot}
\left\{
\begin{array}{ll}
u_t=D\Delta u,& x\in B_0(R),t\in\mathbb R^+,\\
u(x,0)=25 \text{$^{\circ}$},&x\in B_0(R),\\
u(x,t)=100 \text{$^{\circ}$}, &x\in\partial B_0(R), t\in\mathbb R^+.
\end{array}
\right.
\end{equation}
We recognize (\ref{hotpot}) as a homogeneous heat equation with inhomogeneous DBC.  Let us now try to solve this problem.  Without loss of generality, we fix the center of the meatball as the origin.

i) now that the meatball is homogeneous, it is not hard to imagine that each layer has the same temperature with $u(x,t)=u(r,t)$, $r=|x|$.  Show that (\ref{hotpot}) becomes
\begin{equation}\label{hotpotradial}
\left\{
\begin{array}{ll}
u_t=D(\frac{\partial^2u}{\partial r^2}+\frac{2}{r}\frac{\partial u}{\partial r}),& r\in (0,R),t\in\mathbb R^+,\\
u(x,0)=25 \text{$^{\circ}$},&r\in [0,R),\\
u(x,t)=100 \text{$^{\circ}$}, &r=R, t\in\mathbb R^+.
\end{array}
\right.
\end{equation}

ii) we can convert (\ref{hotpotradial}) into a problem with homogeneous DBC by introducing $\mathbb U:=u-100$$^{\circ}$
\begin{equation}\label{hotpotradial2}
\left\{
\begin{array}{ll}
\mathbb U_t=D(\frac{\partial^2\mathbb U}{\partial r^2}+\frac{2}{r}\frac{\partial \mathbb U}{\partial r}),& r\in (0,R),t\in\mathbb R^+,\\
\mathbb U(x,0)=-75 \text{$^{\circ}$},&r\in [0,R),\\
\mathbb U(x,t)=0 \text{$^{\circ}$}, &r=R, t\in\mathbb R^+.
\end{array}
\right.
\end{equation}
To solve (\ref{hotpotradial2}), let us apply the separation of variables by first writing $\mathbb U=\mathbb R(r)\mathbb T(t)$.  Denote $\tilde {\mathbb R}:=r\mathbb R$.  Show that $\tilde {\mathbb R}=\sin\frac{k\pi r}{R}$.  Hint:$\mathbb R(0)<\infty$.

iii) proceed to solve for $u$.  \textit{Suggested answer}:
\[u=100+\sum_{k=1}^\infty \frac{C_k}{r}\sin\frac{k\pi r}{R}e^{-D(\frac{k\pi}{R})^2t}, C_k=?\]

iv) choose $D=0.01$ and $R=1$, then plot the approximate temperature at the center $u^{10}(0,t)$ for $t\in(0,100)$; (we are sloppy with the units, but this gives you an idea how the temperature takes off after a certain time).

\item   Some of you may wonder why we are concerned about and looking for a solution (only) in $L^2$.  This is a very good question and here are some of the main reasons.  First of all, it is obvious that a solution in $L^2$ is not enough, and ultimately we are interested in finding smooth solutions.  However, to this end, you shall find that you lack the tools at this stage.  Therefore, one can first find some solutions in $L^2$, and then prove that (if we can and if they are) they are actually in $L^4$, $L^6$,...$L^\infty$ or other spaces (Sobolev spaces), and then eventually smooth (or not) by techniques called \emph{a prior estimates} or \emph{regularity estimates}.  Of course, other advanced mathematics need to come into play and they are out of the scope of this course.  For example, one of its seven Millennium Prize problems in mathematics proposed by Clay Mathematics Institute in May 2000 is to prove the existence or nonexistence of smooth and globally well--defined solutions to Navier--Stokes equations in 3D, yet it remains open to date.  Second of all, some equations (e.g., wave equations) do not have smooth solutions and may develop shock or singularity in a finite time.  Many reaction-diffusion equations/systems also admit solutions with their $L^\infty$ going to infinity in a finite time, and this is called blow--up in a finite time.  Therefore, the search for a smooth solution for all time $t\in\mathbb R^+,$ is impossible, for many problems.  I also want to mention that, saying that $L^2$ is not enough does not mean it is not of interest, people nowadays are still very interested in finding solutions in $L^p$ spaces, or the so-called weak solutions, however, the method of separation of variables does not work very well, not only from PDE but also because of the properties of the space itself, in particular when the function(solution) has jump discontinuity.   The Eigen--expansion or Fourier expansion of $f(x)$ is the limit of the following sum as $N\rightarrow \infty$
    \[f^N(x):=\sum_{n=0}^\infty C_n X_n(x),\]
    where $\{X_n(x)\}$ is the orthogonal basis of $L^2(0,L)$ and
    \[C_n=\frac{\int_0^L f(x)X_n(x)dx}{\int_0^L X_n^2(x) dx}.\]
  then the general theory states that $f^N\rightarrow f$ pointwise in $(0,L)$ if $f(x)$ is continuous, uniformly if $f(x)$ is differentiable, while in $L^2$ if $f$ is merely square integrable.  In the last case, in particular, when $f(x)$ has jump discontinuity, one may see that the approximation can go wild, and this is usually referred to as the so-called \emph{Gibbs phenomenon}.  To see this yourself, let us consider the following example: let $f(x)$ be a function with a jump at $x=0$ defined over $(-\pi,\pi)$ as: $f(x)=1$ for $x\in[0,\pi)$ and $f(x)=-1$ for $x\in(-\pi,0)$.  First of all, write $f(x)$ into its series as
\begin{equation}\label{cauchy}
      f(x)=\sum_{n=1}^\infty C_n\sin nx.
\end{equation}
    Find $C_n$.  Now let us approximate it by the sum of the first $N$ terms as before
\begin{equation}\label{limit}
     f_N(x):=\sum_{n=1}^N C_n\sin nx
\end{equation}
for some large $N$.  Plot $f_N(x)$ over $(-\pi,\pi)$ for $N=2,4,8,16$ on the same graph.  Try $N=16,32$ and $64$ again.  What are your observations?  You can try with even larger $N$.

\item We recall from baby calculus that a number sequence is called a Cauchy sequence if $|a_{n+1}-a_n|$ is arbitrarily small if $n$ is arbitrarily large; for example $a_n=\frac{1}{n}$ is Cauchy because $|a_{n+1}-a_{n}|=\frac{1}{n(n+1)}\rightarrow 0$ as $n\rightarrow \infty$.  And a region is called \textbf{complete} if every Cauchy converges in it.  For example, $(0,1)$ is not closed since $a_n=\frac{1}{n}$ is Cauchy but $a_n\rightarrow 0\not\in (0,1)$, while $[0,1]$ is closed.  There are more or less rigorous definitions of a closed region.  We have a cousin of closedness when it comes to function space, i.e., completeness.  For a function space $\mathcal X$ endowed with a norm (called a normed space or a metric space) denoted by $\Vert \cdot \Vert_{\mathcal X}$, we say that a function sequence is Cauchy if $\Vert f_{n+1}(x)-f_n(x) \Vert_{\mathcal X}\rightarrow 0$ as $n\rightarrow \infty$.  Here the only difference/generalization is that the absolute value distance is replaced by the so-called ``norm"; then we say that space $\mathcal X$ is \textbf{complete} if every Cauchy sequence converges (in the space or that norm).  For instance, one can prove that $L^p(\Omega)$, $p\in(1,\infty)$ is complete (well, you should have learned in your Analysis course or learned it by yourself otherwise); moreover, in Euclidean space complete is equivalent as closed and bounded.

Going back to the completeness of $L^p$, if $f_n$ is Cauchy in $L^p$ its limit must be in $L^p$.  Then one usually needs to verify the Cauchy-ness of the sequences.

i) prove that the sequence $f_N$ given by (\ref{cauchy}) is Cauchy in $L^2$.  Then according to i), its limit (\ref{limit}) belongs to $L^2$ and this gives you proof of the convergence fashion we had in class, at least for this particular example;

ii) plot $\Vert f_{n+1} -f_n \Vert_{L^2((-\pi,\pi))}$ for $n=1,2,...$ and you should observe the convergence; indeed, to better illustrating the convergence it makes sense to plot the log error $\mathcal E_n:=\log (\Vert f_{n+1} -f_n \Vert_{L^2})$ for $n$ large, for instance starting from $10$.  Remark: MATLAB can evaluate the integrals hence you do not have to do it by brutal force;

iii) try different $L^p$ norms with $p=5,10,20,30$,... and do the same as in ii); plot them in the same graph; what are your observations?

iv) now find the $\max$-norm and do the same as in ii).

\item This problem is to warm up and prepare you for the fashions of convergence of sequences of functions.  The formal way (using $\epsilon-\delta$ language) to define \textbf{pointwise convergence} of $f_n(x)$ over set $E$ is, $\forall x\in E$ and $\forall \epsilon>0$, there exists $N_0\in\mathbb N^+$ such that for all $n\geq N_0$ we have that $\vert f_n(x)-f(x)\vert \leq \epsilon$.  \textbf{Uniform convergence} of $f_n\rightarrow f$ is that, $\forall \epsilon>0$, there exists $N_1\in\mathbb N^+$ such that for all $n\geq N_1$ we have that $\vert f_n(x)-f(x)\vert \leq \epsilon$ $\forall x$ in $E$.  The difference is the order of $\forall x$ and $N$, therefore $N_0$ depends on both $\epsilon$ and $x$, while $N_1$ does not depend on $x$ hence when we say the convergence is uniform we meant that it is uniform with respect to $x$, or the convergence speed does not depend on $x$.

To elaborate, we recall that a function $f(x)$ is \textbf{continuous} at $x_0$ if $\forall \epsilon>0$, we can find $\delta>0$ such that $\vert x-x_0\vert<\delta$ implies $\vert f(x)-f(x_0) \vert<\epsilon$.  Here $\delta$ depends on $\epsilon$ and also $x_0$.  If $\delta$ only depends on $\epsilon$ and is independent of $x_0$, i.e., $\vert x-y\vert<\delta$ implies $\vert f(x)-f(y) \vert<\epsilon$ for any $x,y\in E$, then $f$ is \textbf{uniformly continuous}.  There is another type of continuity called \textbf{equicontinuity} for a sequence of functions $\{f_n(x)\}$, which states that $\vert x-x_0\vert<\delta$ implies $\vert f_n(x)-f_n(x_0) \vert<\epsilon$ for all $n$, while $\delta$ may depend on $x_0$ but does not depend on $n$.  Similarly, if $\delta$ only depends on $\epsilon$, we say that $\{f_n\}$ is \textbf{uniform equicontinuous}.  An important property of equicontinuous function sequence according to \textbf{Arzela-Ascoli} Theorem is that any uniformly bounded and equicontinuous sequence has a subsequence that converges uniformly.  This theorem is one of two cornerstones in modern mathematical analysis (the other being semi-continuity), and you are not required to understand equicontinuity for this course.

We already know that uniform convergence implies pointwise convergence but not the other way.  However, if the limit is also continuous, then pointwise convergence implies uniform convergence (You might be asked to prove this in an advanced analysis course).

(a).  Prove that if a sequence of continuous functions $f_n$ converges to $f$ uniformly in $(a,b)$, then $f$ is continuous.  (Hint: use the $\epsilon-\delta$ language; for each $x_0$ in E, choose $f_n(x_0)$ that converges to $f(x_0)$; do the same for $f_n(x)$ with $x$ being in the neighbourhood of $x_0$.)

(b).  Another important application of uniform convergence is the switch of the order of integral and limit.  Consider the following so-called tent--function
\begin{equation}f_n(x)=
\left\{
\begin{array}{ll}
n^2x,& x\in[0,\frac{1}{n}],\\
2n-n^2x,& x\in(\frac{1}{n},\frac{2}{n}],\\
0,& x\in(\frac{2}{n},1],\\
\end{array}
\right.
\end{equation}
Find the pointwise limit $f(x)$ of $f_n(x)$;  evaluate the limits \[\lim_{n\rightarrow \infty} \int_0^1 f_n(x)dx \text{ and }\int_0^1 \lim_{n\rightarrow \infty} f_n(x)dx.\]  Are they equal?

(c).  What is Lebesgue's Dominated Convergence Theorem? Why does it fail in the example above?

(d).  Let $f_n(x)$ be a sequence that converges to $f(x)$ uniformly on a bounded interval $(a,b)$.  Prove that
\[\lim_{n\rightarrow \infty} \int_a^b f_n(x)dx=\int_a^b \lim_{n\rightarrow \infty} f_n(x)dx=\int_a^b f (x)dx.\]

\end{enumerate}


\end{document}
\endinput
